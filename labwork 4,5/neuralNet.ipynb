{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some activation function and their derivative\n",
    "\n",
    "import math\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "  # understand x as sigmoid(y) itself\n",
    "  return x * (1 - x)\n",
    "\n",
    "def relu(x):\n",
    "  return max(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "  # understand x as relu(y) itself\n",
    "  return 1 if x > 0 else 0\n",
    "\n",
    "def tanh(x):\n",
    "  return math.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "  # understand x as tanh(y) itself\n",
    "  return 1 - x**2\n",
    "\n",
    "derivative_functions = {\n",
    "    'sigmoid': sigmoid_derivative,\n",
    "    'tanh': tanh_derivative,\n",
    "    'relu': relu_derivative\n",
    "}\n",
    "\n",
    "def dot_product(vec1, vec2):\n",
    "  return sum(x * y for x, y in zip(vec1, vec2))\n",
    "\n",
    "def binary_decision(val):\n",
    "  if val >= 0.5:\n",
    "    return 1\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self,id = None, value = random.random()):\n",
    "        # id should be a tuple indexing the neuron in each layer\n",
    "        self.id = id\n",
    "        self.value = value\n",
    "\n",
    "# special class of neuron that will always take value 1 and will not get attach to any neuron in the previous layer\n",
    "class Bias(Neuron):\n",
    "    def __init__(self,id = None):\n",
    "        # id should be a tuple indexing the neuron in each layer\n",
    "        self.id = id\n",
    "        self.value = 1\n",
    "\n",
    "class Layer:\n",
    "    # note: size when input do not count the bias neuron\n",
    "    # but len of layer do count the bias neuron\n",
    "    def __init__(self, id, size, activation_func = sigmoid):\n",
    "        self.id = id\n",
    "        self.activation_func = activation_func\n",
    "        # each layer give it a bias neuron first, even for the ouput layer for convenience\n",
    "        self.neurons = [] # ..................................................................................\n",
    "        count = 1\n",
    "        for neuron in range(size):\n",
    "            self.neurons.append(Neuron((self.id,count)))\n",
    "            count += 1\n",
    "        self.neurons.insert(0,Bias((self.id,0)))\n",
    "\n",
    "    # len do count the bias neuron\n",
    "    def __len__(self):\n",
    "        return len(self.neurons)\n",
    "\n",
    "    # simply perform dot product to use as inputs of next layers\n",
    "    def forward(self, layer_link):\n",
    "      val_vec = [node.value for node in layer_link.from_layer.neurons]\n",
    "      #print(val_vec)\n",
    "      update_vals = []\n",
    "\n",
    "      #Interate through the all the list in layer_link.links\n",
    "      for nodelinks in layer_link.links:\n",
    "        weight_vec = [link.weight for link in nodelinks]\n",
    "        #print(weight_vec)\n",
    "        val = self.activation_func(dot_product(val_vec, weight_vec))\n",
    "        #print(val)\n",
    "        update_vals.append(val)\n",
    "\n",
    "      # update the value for the destination layer, remember to ignore the bias neuron\n",
    "      for i in range(len(update_vals)):\n",
    "        layer_link.to_layer.neurons[i+1].value = update_vals[i]\n",
    "\n",
    "      #print(f\"forward push completed for Layer_link: {layer_link.from_layer.id} to {layer_link.to_layer.id} \")\n",
    "\n",
    "\n",
    "class Link:\n",
    "    def __init__(self,source,desti,weight = random.random()):\n",
    "        # source and destination should be neurons\n",
    "        self.source = source\n",
    "        self.desti = desti\n",
    "        self.weight = weight\n",
    "\n",
    "\n",
    "class LayerLink:\n",
    "    def __init__(self, from_layer, to_layer, weights = None):\n",
    "        self.from_layer = from_layer\n",
    "        self.to_layer = to_layer\n",
    "        self.links = []\n",
    "        # the structure of self.links should be consist of list of links from all neurons in from_layer to a particular neuron in to_layer\n",
    "        # this make it convenient for forward() later\n",
    "        # remember not to connect with the bias neuron of the next layer\n",
    "        for i in range(1,len(to_layer)):\n",
    "            nodelinks = []\n",
    "            for j in range(len(from_layer)):\n",
    "                link = Link(from_layer.neurons[j],to_layer.neurons[i])\n",
    "                nodelinks.append(link)\n",
    "                #print(f\"Create link from {link.source.id} to {link.desti.id}\\n\")\n",
    "            self.links.append(nodelinks)\n",
    "        if weights != None:\n",
    "          self.set_weights(weights)\n",
    "\n",
    "    # set weight accordingly to the structure of self.links\n",
    "    def set_weights(self,list_list_weights):\n",
    "      for i in range(len(list_list_weights)):\n",
    "        list_weights = list_list_weights[i]\n",
    "        for j in range(len(list_weights)):\n",
    "          self.links[i][j].weight = list_list_weights[i][j]\n",
    "          #print(f\"Value of weight from {self.links[i][j].source.id} to {self.links[i][j].desti.id} is:{list_list_weights[i][j]}\\n\")\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, learn_rate = 0.001):\n",
    "        # Layer 0,1,2,3...n\n",
    "        self.layers = []\n",
    "        self.depth = 0\n",
    "        # Layer link 0-->1, 1-->2, ..., n-1 -->n\n",
    "        self.layer_links = []\n",
    "\n",
    "        self.dlossda = dict()\n",
    "        self.dlossdw = dict()\n",
    "        self.learn_rate =  learn_rate\n",
    "\n",
    "    def add_layer(self, size):\n",
    "        new_layer = Layer(self.depth,size)\n",
    "        self.layers.append(new_layer)\n",
    "        self.depth += 1\n",
    "        # Input layer dont need to do this\n",
    "        if self.depth > 1:\n",
    "          # Hidden layers and output layer\n",
    "          # this is the layer_link that connect previous layer --> this new layer\n",
    "          # -2 because remember depth = 1 correspond to layer 0 in the list\n",
    "          layer_link = LayerLink(self.layers[self.depth-2], new_layer)\n",
    "          self.layer_links.append(layer_link)\n",
    "\n",
    "    def get_weight(self,from_layer_id,from_neuron_id,to_neuron_id):\n",
    "      return self.layer_links[from_layer_id][to_neuron_id][from_neuron_id]\n",
    "\n",
    "    def set_weight(self,from_layer_id,from_neuron_id,to_neuron_id,val):\n",
    "      self.layer_links[from_layer_id][to_neuron_id][from_neuron_id] = val\n",
    "\n",
    "    # forward from the input layer to the output layer. Return the value of output layer\n",
    "    def forward(self, inputs):\n",
    "        # check valid input.. -1 because dont count the bias\n",
    "        if len(inputs) != len(self.layers[0])-1:\n",
    "            print(\"invalid input\")\n",
    "\n",
    "        # initialize the input layer neurons with the input\n",
    "        for i in range(len(inputs)) :\n",
    "            self.layers[0].neurons[i+1].value = inputs[i] # +1 to ignore the bias\n",
    "\n",
    "        # invoke the forward function of each layer (except for the last layer)\n",
    "        for layer in self.layers[:-1]:\n",
    "            layer.forward(self.layer_links[layer.id])\n",
    "\n",
    "        # remember to ignore the bias neuron\n",
    "        output = [neuron.value for neuron in self.layers[-1].neurons[1:]]\n",
    "        return output\n",
    "\n",
    "    # notation w,a based on the diagram above\n",
    "    def back_propagation(self,loss_function, step_size = 0.01):\n",
    "      self.dlossda = dict() # clear the gradient value)\n",
    "      self.dlossdw = dict()\n",
    "\n",
    "      loss_deri = derivative_functions[loss_function]\n",
    "\n",
    "      final_output = [neuron.value for neuron in self.layers[-1].neurons[1:]]\n",
    "      dlossdoutput = loss_deri(*final_output) #Passing a List as Arguments\n",
    "      # assume that the is only one neuron in the output layer, so that dloss/doutput is a number\n",
    "      self.dlossda[self.layers[-1].neuron[1].id] = dlossdoutput\n",
    "\n",
    "      # start backprop, calc the derivative of all weight wrt to loss\n",
    "      for i in range(self.depth,1,-1): # no backprop in the input layer\n",
    "        for j in range(1,len(self.layers[i])): # no backprop for bias node of to_layer\n",
    "          for k in range(0,len(self.layers[i-1])): # iterate for from_layer neurons\n",
    "            self.dlossdw[(i-1,k,j)] = self.dlossda[(i,j)] * derivative_functions[self.layers[i].activation_func](self.layers[i-1].neurons[k].value)*self.layers[i].neurons[j].value\n",
    "\n",
    "            # calc dloss/da also to calc gradient of next layer\n",
    "            if (i-1,k) in self.dlossda:\n",
    "              # If the key exists, increment its value\n",
    "              self.dlossda[(i-1,k)] += self.dlossda[(i,j)] * derivative_functions[self.layers[i].activation_func](self.layers[i-1].neurons[k].value)*self.get_weight(i-1,k,j)\n",
    "            else:\n",
    "              # If the key does not exist, initialize it\n",
    "              self.dlossda[(i-1,k)]  = self.dlossda[(i,j)] * derivative_functions[self.layers[i].activation_func](self.layers[i-1].neurons[k].value)*self.get_weight(i-1,k,j)\n",
    "\n",
    "\n",
    "      # start updating weights (once for each weight)\n",
    "      for key in self.dlossdw.keys():\n",
    "        self.set_weight(*key, self.get_weight(*key) + self.dlossdw[key]* self.learning_rate)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
