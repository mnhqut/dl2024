{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # 1 here is the input channel size of previous layer ....\n",
    "            # original has 96 kernels with stride = 4 and size 11\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # Max pooling layer with a 2x2 kernel size and a default stride of 2\n",
    "            # origal has size 3x3 and stride 2\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            # original has 256 kernels with stride = 1 and size 5\n",
    "            # 64 here is the input channel size of previous layer ....\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # origal has size 3x3 and stride 2\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            # original has 384 kernels with stride = 1 and size 3\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # original has 384 kernels with stride = 1 and size 3\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # original has 256 kernels with stride = 1 and size 3\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # original has size 3x3 and stride 2\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        # This layer will adaptively resize the input tensor to the specified size (6x6),\n",
    "        # performing average pooling to produce a fixed-size output regardless of the input size.\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "\n",
    "        # Now move to the fully connected layer\n",
    "        # just the same as feed-forward neural network\n",
    "        # original alexnet has 2 hidden layer, each has 4096 neurons, so we keep the same here\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Dropout layer to randomly zero some of the elements of the input tensor with a probability of 0.5.\n",
    "            # Dropout is a regularization technique used to prevent overfitting by randomly dropping units during training.\n",
    "            nn.Dropout(),\n",
    "\n",
    "            #Fully connected (linear) layer with 256*6*6 input features (output size from the last convolutional layer)\n",
    "            # and 4096 output features.\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "            # no soft max activation function for the final output layer ??\n",
    "            # somehow the result (see below) is still really good\n",
    "            # maybe i just used too strong of a model for a simple task\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        # flatten before feeding in the fully connected layer\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        probabilities = F.softmax(x, dim=1)  # Apply softmax\n",
    "        return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AlexNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "def train_model(num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "num_epochs = 10\n",
    "train_model(num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the model on the 10000 test images: {100 * correct / total}%')\n",
    "\n",
    "evaluate_model()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
